"pull_number","event_number","event","actor","time","state","commit_id","referenced","sha","title","body","html_url","is_bot","is_chatgpt","is_first_chatgpt","is_proceeding_chatgpt","chatgpt_event"
"13","0","committed","ishaan-jaff","2025-01-01 18:12:58","","","False","59f21a2a18da17c725210fdf23287b9e37970ef8","<NA>","<NA>","https://github.com/ServiceNow/context-is-key-forecasting/commit/59f21a2a18da17c725210fdf23287b9e37970ef8","False","False","False","False",""
"13","1","pulled","ishaan-jaff","2025-01-01 18:13:10","open","","False","","Add LiteLLM - support for Vertex AI, Gemini, Anthropic, Bedrock (100+LLMs)","# Add LiteLLM - support for Vertex AI, Gemini, Anthropic, Bedrock (100+LLMs)

# What's changing

This PR adds support for the above mentioned LLMs using LiteLLM https://github.com/BerriAI/litellm/ 
LiteLLM is a lightweight package to simplify LLM API calls - use any llm as a drop in replacement for gpt-4o.

Example

```python
from litellm import completion
import os

## set ENV variables
os.environ[""OPENAI_API_KEY""] = ""your-openai-key""
os.environ[""ANTHROPIC_API_KEY""] = ""your-cohere-key""

messages = [{ ""content"": ""Hello, how are you?"",""role"": ""user""}]

# openai call
response = completion(model=""openai/gpt-4o"", messages=messages)

# anthropic call
response = completion(model=""anthropic/claude-3-sonnet-20240229"", messages=messages)
print(response)
```

### Response (OpenAI Format)

```json
{
    ""id"": ""chatcmpl-565d891b-a42e-4c39-8d14-82a1f5208885"",
    ""created"": 1734366691,
    ""model"": ""claude-3-sonnet-20240229"",
    ""object"": ""chat.completion"",
    ""system_fingerprint"": null,
    ""choices"": [
        {
            ""finish_reason"": ""stop"",
            ""index"": 0,
            ""message"": {
                ""content"": ""Hello! As an AI language model, I don't have feelings, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?"",
                ""role"": ""assistant"",
                ""tool_calls"": null,
                ""function_call"": null
            }
        }
    ],
    ""usage"": {
        ""completion_tokens"": 43,
        ""prompt_tokens"": 13,
        ""total_tokens"": 56,
        ""completion_tokens_details"": null,
        ""prompt_tokens_details"": {
            ""audio_tokens"": null,
            ""cached_tokens"": 0
        },
        ""cache_creation_input_tokens"": 0,
        ""cache_read_input_tokens"": 0
    }
}
```

","https://github.com/ServiceNow/context-is-key-forecasting/pull/13","False","True","True","False",""
"13","2","commented","ishaan-jaff","2025-01-01 18:14:09","","","False","","<NA>","@aldro61 @AndrewRWilliams can I get a review on this PR ? ","https://github.com/ServiceNow/context-is-key-forecasting/pull/13#issuecomment-2567099880","False","False","False","False",""
"13","3","mentioned","aldro61","2025-01-01 18:14:10","","","False","","<NA>","<NA>","","False","False","False","True","1"
"13","4","subscribed","aldro61","2025-01-01 18:14:10","","","False","","<NA>","<NA>","","False","False","False","False",""
"13","5","mentioned","andrewrwilliams","2025-01-01 18:14:10","","","False","","<NA>","<NA>","","False","False","False","False",""
"13","6","subscribed","andrewrwilliams","2025-01-01 18:14:10","","","False","","<NA>","<NA>","","False","False","False","False",""
"13","7","renamed","ishaan-jaff","2025-01-01 18:15:31","","","False","","<NA>","<NA>","","False","False","False","False",""
"13","8","commented","ashok-arjun","2025-01-06 21:38:14","","","False","","<NA>","@ishaan-jaff Thank you for this PR!!

A few things:
1. Can you also add `litellm` to the requirements?
2. If some models require new API keys (such as an Anthropic API key), can you add that to the [environment variables list](https://github.com/ServiceNow/context-is-key-forecasting?tab=readme-ov-file#setting-environment-variables) in README.md? Please prepend LiteLLM-specific keys with `CIK_LITELLM_` so it'll be easier to distinguish those new keys.
3. Have you tested if this works, with some LLMs? I'm very curious to know if any specific LLMs' performance stood out.

Thanks again!","https://github.com/ServiceNow/context-is-key-forecasting/pull/13#issuecomment-2573972443","False","False","False","False",""
"13","9","mentioned","ishaan-jaff","2025-01-06 21:38:15","","","False","","<NA>","<NA>","","False","False","False","False",""
"13","10","subscribed","ishaan-jaff","2025-01-06 21:38:15","","","False","","<NA>","<NA>","","False","False","False","False",""
"13","11","assigned","ashok-arjun","2025-01-06 21:38:19","","","False","","<NA>","<NA>","","False","False","False","False",""
